{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuhin Mahmud**\n",
    "\n",
    "tuihinm@ischool.berkeley.edu    \n",
    "W261-1 spring 2016  \n",
    "Week 1: Homework\n",
    "\n",
    "date:01/21/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.0:  Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n",
    "Big data is defined in terms of four **V**s i.e *volume, volicity, variety, veracity*. Any data that is large in all or most of these criteria are considered big data. Data is Big Data when it cannot be stored on a single device e.g our laptop or single workstation machine and/or cannot be handled in a timely manner without large-sized parallel processing.\n",
    "\n",
    "For volume it big is considered to be  peta or zeta byte thats but it changes by context and use and increasing becoming larger.\n",
    " \n",
    "I work in Verification Simulation tool and our backend result data is growing exponentially with each chip generation .Our daily volume of results are now in the terabyte range with thousands of simulation runs. This volume is becoming increasingly unmanagable with the traditional tools and we are considering using big data tools like Apache hadoop and spark to collect and analyze the results. Current stage of development is to set up the cluster and build the infrastructure needed. We are exploring soft layer offering in this regard as well as builing the infrastructure from hardware machines available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.1\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.0.1:  \n",
    "\n",
    "### In 500 words (English or pseudo code or a combination),  describe how to estimate the bias, the variance, and the error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5. How would you select a model?   \n",
    "   \n",
    "\n",
    "**Step 1: Create test, training, and validation datasets**    \n",
    "Randomly separate the given dataset ***T*** into a test dataset ***A*** (20% of the data) and a training dataset ***B*** (remaining 80% of the data).  Next, create *S* training and validation datasets from ***B*** using the sub-sampled cross-valida\n",
    "tion (ssCV) \n",
    "\n",
    "     \n",
    "    \n",
    "**Step 2:  Build models**  \n",
    "For each training set $train_s$, build a model $h_s^p(x) = \\beta_0 + \\beta_1x^1 + … + \\beta_px^p$   for p = 1, … , 5.  \n",
    "   \n",
    "**Step 3:  Estimate Bias**  \n",
    "For each validation set $valid_s$, calculate the squared bias of $h_s^p(x)$ for p = 1, ... , 5 using the formula:  $$Bias_s^p = \\frac{1}{N}\\sum_{i=1}^N[h_s^p(x_i)-y_i]^2$$ where $(x_i, y_i) \\in valid_s$.  \n",
    "   \n",
    "Estimate the overall bias for p = 1, ... , 5 by calculating the mean of the biases above using the formula:  $$Bias^p = \\frac{1}{S}\\sum_{s=1}^SBias_s^p$$\n",
    "   \n",
    "**Step 4:  Estimate Variance**  \n",
    "Estimate the variances for p = 1, ... , 5 by calculating: $$Variance^p = \\frac{1}{S}\\sum_{s=1}^S\\frac{1}{N_s}\\sum_{i=1}^{N_s}[h_s^p(x_{si})-\\frac{1}{S}\\sum_{t=1}^Sh_t^p(x_{si})]^2$$ where $x_{si} \\in valid_s$.\n",
    "   \n",
    "**Step 5:  Estimate Error**  \n",
    "Estimate the model's mean squared error for p = 1, ... , 5 using the test dataset ***A*** by calculating:  $$MSE^p = \\frac{1}{N}\\sum_{i=1}^N[\\frac{1}{S}\\sum_{s=1}^Sh_s^p(x_i) - y_i]^2$$ where $(x_i, y_i) \\in$ ***A***.\n",
    "  \n",
    "**Step 6:  Select a model**   \n",
    "To select a model, choose the polynomial with the lowest *MSE*.\n",
    "\n",
    "**Overall Bias vs Variance Trade off**\n",
    "\n",
    "Following grarph depicts the bias vs variance trade off that is typical of model selection process as different model complexity is considered. The same overall trade off is considered in the 6 steps algorithm described above.\n",
    "\n",
    "<img src=\"biasVsVarianceTradeOff.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\"\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "#echo $linesinchunk\n",
    "\n",
    "#exit\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "   ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "## pass the number of mapper outfile and wordlist to the reducer\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def hw1_1():\n",
    "    print 'Done.'\n",
    "    return\n",
    "\n",
    "hw1_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HW1.2\n",
    "-------------\n",
    "### Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "def mapper(fname,mlist):\n",
    "    #print mlist\n",
    "    wordCounter = Counter()\n",
    "    for line in open(fname):\n",
    "        line = line.strip()\n",
    "        words = line.split()\n",
    "        id =line.split('\\t')[0]\n",
    "        for word in words:\n",
    "            # remove any trailing punctuations from the word\n",
    "            nword = word.rstrip('?:!.,;')\n",
    "            #if the list of words provided is empty consider all \n",
    "            if not mlist:\n",
    "                wordCounter.update({nword:1})\n",
    "            else:\n",
    "               #find the word match\n",
    "               if nword in mlist:\n",
    "                   wordCounter.update({nword:1})\n",
    "                   #print \"id=\",id,\"word\",word,\"count\",wordCounter[nword]\n",
    "    #print wordCounter\n",
    "    for word in wordCounter:\n",
    "        print word,\"\\t\",wordCounter[word]\n",
    "def main():\n",
    "    if len(sys.argv) < 3:\n",
    "        print \"incorrect number of arguments\"\n",
    "        return\n",
    "    fname=sys.argv[1]\n",
    "    wordlist=sys.argv[2].lower()\n",
    "    c = Counter()\n",
    "    if sys.argv[2] == \"*\":\n",
    "        wordlist=[]\n",
    "    mapper(fname,wordlist.split())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "def reducer(countfiles):\n",
    "    #print countfiles\n",
    "    wordCounter = Counter()\n",
    "    for fname in countfiles:\n",
    "        current_word = None\n",
    "        current_count = 0\n",
    "        word = None\n",
    "        for line in open(fname):\n",
    "            line = line.strip()\n",
    "            word, count = line.split('\\t', 1)\n",
    "\n",
    "            try:\n",
    "                count = int(count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            #print word,count\n",
    "            wordCounter.update({word:count})\n",
    "    #print wordCounter\n",
    "    for word in wordCounter:\n",
    "        print word,\"\\t\",wordCounter[word]\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print \"%s wrong number of arguments\" % sys.argv\n",
    "        return\n",
    "    countfiles=[]\n",
    "    countfiles=sys.argv[1:]\n",
    "    reducer(countfiles)\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MapReduce 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance  \t10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def hw1_2():\n",
    "    !./pNaiveBayes.sh 2 \"assistance\"\n",
    "\n",
    "    # print out results\n",
    "    with open (\"enronemail_1h.txt.output\", \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            print line\n",
    "    return\n",
    "\n",
    "hw1_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.3\n",
    "-----------\n",
    "###  Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a single, user-specified word. Examine the word “assistance” and report your results. To do so, make sure that (a) mapper.py is the same as in part (2), and (b) reducer.py performs a single word Naive Bayes classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pNaiveBayesh.sh 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\"\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "#echo $linesinchunk\n",
    "\n",
    "#exit\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "   ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "## pass the number of mapper outfile and wordlist to the reducer\n",
    "#num_countfiles=`echo \"$linesindata/($linesindata/$m+1)+1\" | bc`\n",
    "#./reducer.py $num_countfiles \"$wordlist\" $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper 1.3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "debugFile=open (\"debug.mapper.out\", \"w\")\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "def mapper(filename,findwords):\n",
    "    findwords = [item.lower() for item in findwords] # make all findwords lowercase\n",
    "    findwordsSet = set(findwords) # create a set of unique find words\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            components = line.split('\\t')\n",
    "            if len(components) < 3:\n",
    "                continue\n",
    "            ID = components[0]\n",
    "            flag = components[1]\n",
    "            text = \" \".join(components[2:])\n",
    "            wordCounter = Counter()\n",
    "            for word in WORD_RE.findall(text):\n",
    "                print >>debugFile,line\n",
    "                wordCounter.update({word:1})\n",
    "                #print >>debugFile, word,word_count[word]c\n",
    "            for word in wordCounter:\n",
    "                found_word = 0\n",
    "                if word.lower() in findwordsSet:\n",
    "                    found_word = 1\n",
    "                print \"%s\\t%s\\t%s\\t%s\\t%s\" % (ID,str(flag),word,str(wordCounter[word]),str(found_word))\n",
    "\n",
    "def main():\n",
    "    fname=sys.argv[1]\n",
    "    findwords=sys.argv[2].split()\n",
    "    mapper(fname,findwords)\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer 1.3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "debugFile =open(\"debug.reducer.out\",\"w\")\n",
    "\n",
    "\n",
    "def reducer(files,laplace):\n",
    "\n",
    "    #spam/ham related data structure to keep counts\n",
    "    vocab = set()\n",
    "\n",
    "    spamCount = 0 # number of vocab words that are in spam\n",
    "    hamCount= 0  # number of vocab words that are in spam\n",
    "    spam =set() # unique set of email IDs that are in spam\n",
    "    ham=set()    # unique set of email IDs that are in ham\n",
    "    spamWordCounter = Counter() # counter for each vocab word that are spam\n",
    "    hamWordCounter = Counter() # counter for each vocab word that are ham\n",
    "    condProbSpam = Counter()\n",
    "    condProbHam = Counter()\n",
    "\n",
    "    for filename in files:\n",
    "        with open(filename, 'r') as myfile:\n",
    "            for line in myfile.readlines():\n",
    "                ID,flag,word,count,findword = line.split('\\t')\n",
    "                print >>debugFile, ID,flag,word,count,findword\n",
    "                flag =int(flag)\n",
    "                count=int(count)\n",
    "                findword=int(findword)\n",
    "                if findword == 1:\n",
    "                    print >>debugFile, \"findword==1\" ,ID,flag,word,count,findword\n",
    "                    vocab.add(word)\n",
    "                if flag == 1:\n",
    "                    spam.add(ID) # save unique IDs of emails in spam\n",
    "                    spamCount += count\n",
    "                    if findword == 1:\n",
    "                        spamWordCounter.update({word:count})\n",
    "                else:\n",
    "                    ham.add(ID)  # save unique IDs of emails in ham\n",
    "                    hamCount += count\n",
    "                    if findword == 1:\n",
    "                        hamWordCounter.update({word:count})\n",
    "                    #print >>debugFile, spamCount,hamCount,word,spamWordCounter[word],hamWordCounter[word]\n",
    "\n",
    "    # calculate prior probabilities\n",
    "    count_ham = len(ham)\n",
    "    count_spam = len(spam)\n",
    "    prior_Pr_ham = count_ham*1.0/(count_ham + count_spam)\n",
    "    prior_Pr_spam = count_spam*1.0/(count_ham + count_spam)\n",
    "\n",
    "\n",
    "    hamWordCount=len(hamWordCounter)\n",
    "    spamWordCount=len(spamWordCounter)\n",
    "\n",
    "    #print >>debugFile,vocab\n",
    "    B_value = len(vocab)\n",
    "    for v in vocab:\n",
    "        if (laplace==1) :\n",
    "            #laplace smoothing\n",
    "            value =float(spamWordCounter[v]+1)/float(spamCount + B_value)\n",
    "            condProbSpam.update({v:value})\n",
    "            value =float(hamWordCounter[v]+1)/float(hamCount + B_value)\n",
    "            condProbHam.update({v:value})\n",
    "        else:\n",
    "            value =float(spamWordCounter[v])/float(spamCount)\n",
    "            condProbSpam.update({v:value})\n",
    "            value =float(hamWordCounter[v])/float(hamCount)\n",
    "            condProbHam.update({v:value})\n",
    "\n",
    "    ## run test on the full file using the training value found above\n",
    "    \n",
    "    output=[]\n",
    "    WORD_RE = re.compile(r\"[\\w']+\")\n",
    "    with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    #with open('test.txt', 'r') as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            print >>debugFile, line\n",
    "            components = line.split('\\t')\n",
    "            if len(components) < 3:\n",
    "                continue\n",
    "\n",
    "            ID = components[0]\n",
    "            trueClass = components[1]\n",
    "            text = \" \".join(components[2:]) # use botht the subject and text\n",
    "\n",
    "            log_post_Pr_spam = np.log(prior_Pr_spam)\n",
    "            log_post_Pr_ham = np.log(prior_Pr_ham)\n",
    "\n",
    "            for word in WORD_RE.findall(text):\n",
    "                if word in vocab :\n",
    "                    value = condProbSpam[v]\n",
    "                    if ( laplace != 1):\n",
    "                        if condProbSpam[word] == 0:\n",
    "                            value=0.0000000001 # set it to a small number instead of 0                        \n",
    "                            #continue\n",
    "                    log_post_Pr_spam  += np.log(value)\n",
    "                if word in vocab:\n",
    "                    value = condProbHam[word]\n",
    "                    if ( laplace != 1):\n",
    "                        if condProbHam[word] == 0:\n",
    "                            value=0.0000000001 # set it to a small number instead of 0\n",
    "                            #continue\n",
    "                    log_post_Pr_ham  += np.log(value)\n",
    "\n",
    "            #predict the email class based on which          \n",
    "            predicted = 0\n",
    "            if  log_post_Pr_spam  >  log_post_Pr_ham :\n",
    "                predicted = 1\n",
    "            outputline = \"%s\\t%s\\t%s\" % (ID ,str(trueClass),str(predicted))\n",
    "            output.append(outputline)\n",
    "            \n",
    "    print \"%s\\t%s\\t%s\" % (\"id\",\"actual\",\"predicted\")\n",
    "    for entry_line in output:\n",
    "        print entry_line\n",
    "\n",
    "\n",
    "def main():\n",
    "    if len(sys.argv) < 2:\n",
    "        print \"%s wrong number of arguments\" % sys.argv\n",
    "        return\n",
    "    countfiles=[]\n",
    "    countfiles=sys.argv[1:]\n",
    "    reducer(countfiles,0)\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function used to read teh output from reducer.py and find the accuracy of the prediction\n",
    "\n",
    "def find_accuracy(df):\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    for index, row in df.iterrows():\n",
    "        #print row['actual'],row['predicted']\n",
    "        try:\n",
    "            actual_value =int(row['actual'])\n",
    "        except:\n",
    "            continue\n",
    "        try:\n",
    "            predicted_value =int(row['predicted'])\n",
    "        except:\n",
    "            continue   \n",
    "            \n",
    "        if actual_value == predicted_value:\n",
    "            #print \"found correct\"\n",
    "            correct += 1\n",
    "        count += 1\n",
    "    accuracy = correct*1.0/count\n",
    "    print \"prediction accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MapReduce 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 0.59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.1999-12-10.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002.1999-12-13.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0002.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0002.2001-05-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0002.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0003.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0003.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0003.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0003.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0003.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0004.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0004.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0004.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0004.2001-06-12.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0004.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0005.1999-12-12.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0005.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0005.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0005.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0005.2001-06-23.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0005.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0006.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0006.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0006.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0006.2001-06-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0006.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0006.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0007.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0007.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0007.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0007.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0007.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0007.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0008.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0008.2001-06-12.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0008.2001-06-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0008.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0008.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0009.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0009.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0009.2000-06-07.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0009.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0009.2001-06-26.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0009.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0010.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0010.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0010.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0010.2001-06-28.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0010.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0010.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0011.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0011.2001-06-28.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0011.2001-06-29.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0011.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0011.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0012.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0012.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0012.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0012.2000-06-08.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0012.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0012.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0013.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0013.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0013.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0013.2001-06-30.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0013.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0014.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0014.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0014.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0014.2001-07-04.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0014.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0014.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0015.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0015.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0015.2000-06-09.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0015.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0015.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0015.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0016.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0016.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0016.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0016.2001-07-06.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0016.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0016.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0017.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0017.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0017.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0017.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0017.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0017.2004-08-02.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0018.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0018.2001-07-13.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0018.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  actual  predicted\n",
       "0      0001.1999-12-10.farmer       0          0\n",
       "1    0001.1999-12-10.kaminski       0          0\n",
       "2        0001.2000-01-17.beck       0          0\n",
       "3       0001.2000-06-06.lokay       0          0\n",
       "4     0001.2001-02-07.kitchen       0          0\n",
       "5    0001.2001-04-02.williams       0          0\n",
       "6      0002.1999-12-13.farmer       0          0\n",
       "7     0002.2001-02-07.kitchen       0          0\n",
       "8   0002.2001-05-25.SA_and_HP       1          0\n",
       "9          0002.2003-12-18.GP       1          0\n",
       "10         0002.2004-08-01.BG       1          1\n",
       "11   0003.1999-12-10.kaminski       0          0\n",
       "12     0003.1999-12-14.farmer       0          0\n",
       "13       0003.2000-01-17.beck       0          0\n",
       "14    0003.2001-02-08.kitchen       0          0\n",
       "15         0003.2003-12-18.GP       1          0\n",
       "16         0003.2004-08-01.BG       1          0\n",
       "17   0004.1999-12-10.kaminski       0          1\n",
       "18     0004.1999-12-14.farmer       0          0\n",
       "19   0004.2001-04-02.williams       0          0\n",
       "20  0004.2001-06-12.SA_and_HP       1          0\n",
       "21         0004.2004-08-01.BG       1          0\n",
       "22   0005.1999-12-12.kaminski       0          1\n",
       "23     0005.1999-12-14.farmer       0          0\n",
       "24      0005.2000-06-06.lokay       0          0\n",
       "25    0005.2001-02-08.kitchen       0          0\n",
       "26  0005.2001-06-23.SA_and_HP       1          0\n",
       "27         0005.2003-12-18.GP       1          0\n",
       "28   0006.1999-12-13.kaminski       0          0\n",
       "29    0006.2001-02-08.kitchen       0          0\n",
       "30   0006.2001-04-03.williams       0          0\n",
       "31  0006.2001-06-25.SA_and_HP       1          0\n",
       "32         0006.2003-12-18.GP       1          0\n",
       "33         0006.2004-08-01.BG       1          0\n",
       "34   0007.1999-12-13.kaminski       0          0\n",
       "35     0007.1999-12-14.farmer       0          0\n",
       "36       0007.2000-01-17.beck       0          0\n",
       "37    0007.2001-02-09.kitchen       0          0\n",
       "38         0007.2003-12-18.GP       1          0\n",
       "39         0007.2004-08-01.BG       1          0\n",
       "40    0008.2001-02-09.kitchen       0          0\n",
       "41  0008.2001-06-12.SA_and_HP       1          0\n",
       "42  0008.2001-06-25.SA_and_HP       1          0\n",
       "43         0008.2003-12-18.GP       1          0\n",
       "44         0008.2004-08-01.BG       1          0\n",
       "45   0009.1999-12-13.kaminski       0          0\n",
       "46     0009.1999-12-14.farmer       0          0\n",
       "47      0009.2000-06-07.lokay       0          0\n",
       "48    0009.2001-02-09.kitchen       0          0\n",
       "49  0009.2001-06-26.SA_and_HP       1          0\n",
       "50         0009.2003-12-18.GP       1          0\n",
       "51     0010.1999-12-14.farmer       0          0\n",
       "52   0010.1999-12-14.kaminski       0          0\n",
       "53    0010.2001-02-09.kitchen       0          0\n",
       "54  0010.2001-06-28.SA_and_HP       1          1\n",
       "55         0010.2003-12-18.GP       1          0\n",
       "56         0010.2004-08-01.BG       1          0\n",
       "57     0011.1999-12-14.farmer       0          0\n",
       "58  0011.2001-06-28.SA_and_HP       1          0\n",
       "59  0011.2001-06-29.SA_and_HP       1          0\n",
       "60         0011.2003-12-18.GP       1          0\n",
       "61         0011.2004-08-01.BG       1          0\n",
       "62     0012.1999-12-14.farmer       0          0\n",
       "63   0012.1999-12-14.kaminski       0          0\n",
       "64       0012.2000-01-17.beck       0          0\n",
       "65      0012.2000-06-08.lokay       0          0\n",
       "66    0012.2001-02-09.kitchen       0          0\n",
       "67         0012.2003-12-19.GP       1          0\n",
       "68     0013.1999-12-14.farmer       0          0\n",
       "69   0013.1999-12-14.kaminski       0          0\n",
       "70   0013.2001-04-03.williams       0          0\n",
       "71  0013.2001-06-30.SA_and_HP       1          0\n",
       "72         0013.2004-08-01.BG       1          1\n",
       "73   0014.1999-12-14.kaminski       0          0\n",
       "74     0014.1999-12-15.farmer       0          0\n",
       "75    0014.2001-02-12.kitchen       0          0\n",
       "76  0014.2001-07-04.SA_and_HP       1          0\n",
       "77         0014.2003-12-19.GP       1          0\n",
       "78         0014.2004-08-01.BG       1          0\n",
       "79   0015.1999-12-14.kaminski       0          0\n",
       "80     0015.1999-12-15.farmer       0          0\n",
       "81      0015.2000-06-09.lokay       0          0\n",
       "82    0015.2001-02-12.kitchen       0          0\n",
       "83  0015.2001-07-05.SA_and_HP       1          0\n",
       "84         0015.2003-12-19.GP       1          0\n",
       "85     0016.1999-12-15.farmer       0          0\n",
       "86    0016.2001-02-12.kitchen       0          0\n",
       "87  0016.2001-07-05.SA_and_HP       1          0\n",
       "88  0016.2001-07-06.SA_and_HP       1          0\n",
       "89         0016.2003-12-19.GP       1          0\n",
       "90         0016.2004-08-01.BG       1          0\n",
       "91   0017.1999-12-14.kaminski       0          0\n",
       "92       0017.2000-01-17.beck       0          0\n",
       "93   0017.2001-04-03.williams       0          0\n",
       "94         0017.2003-12-18.GP       1          0\n",
       "95         0017.2004-08-01.BG       1          0\n",
       "96         0017.2004-08-02.BG       1          0\n",
       "97   0018.1999-12-14.kaminski       0          0\n",
       "98  0018.2001-07-13.SA_and_HP       1          1\n",
       "99         0018.2003-12-18.GP       1          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "def hw1_3():\n",
    "    !./pNaiveBayes.sh 10 \"assistance\"\n",
    "    \n",
    "    df = pd.read_csv(\"enronemail_1h.txt.output\",sep='\\t')\n",
    "    find_accuracy(df)\n",
    "    with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):\n",
    "        display(df)\n",
    "    return\n",
    "\n",
    "hw1_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.4\n",
    "-----------\n",
    "### Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by a list of one or more user-specified words.  Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.  To do so, make sure that (a) mapper.py counts all occurrences of a list of words, and (b) reducer.py performs the multiple-word Naive Bayes classification via the chosen list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer 1.4\n",
    "---\n",
    " * Uses Laplace smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MapReduce 1.4\n",
    "---\n",
    "* Improved accuracy from 1.3 from use of multiple words\n",
    "* uses Laplace smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 0.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.1999-12-10.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002.1999-12-13.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0002.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0002.2001-05-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0002.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0003.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0003.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0003.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0003.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0003.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0004.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0004.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0004.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0004.2001-06-12.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0004.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0005.1999-12-12.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0005.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0005.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0005.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0005.2001-06-23.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0005.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0006.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0006.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0006.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0006.2001-06-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0006.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0006.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0007.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0007.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0007.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0007.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0007.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0007.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0008.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0008.2001-06-12.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0008.2001-06-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0008.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0008.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0009.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0009.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0009.2000-06-07.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0009.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0009.2001-06-26.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0009.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0010.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0010.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0010.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0010.2001-06-28.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0010.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0010.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0011.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0011.2001-06-28.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0011.2001-06-29.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0011.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0011.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0012.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0012.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0012.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0012.2000-06-08.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0012.2001-02-09.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0012.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0013.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0013.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0013.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0013.2001-06-30.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0013.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0014.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0014.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0014.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0014.2001-07-04.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0014.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0014.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0015.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0015.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0015.2000-06-09.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0015.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0015.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0015.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0016.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0016.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0016.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0016.2001-07-06.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0016.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0016.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0017.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0017.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0017.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0017.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0017.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0017.2004-08-02.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0018.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0018.2001-07-13.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0018.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           id  actual  predicted\n",
       "0      0001.1999-12-10.farmer       0          0\n",
       "1    0001.1999-12-10.kaminski       0          0\n",
       "2        0001.2000-01-17.beck       0          0\n",
       "3       0001.2000-06-06.lokay       0          0\n",
       "4     0001.2001-02-07.kitchen       0          0\n",
       "5    0001.2001-04-02.williams       0          0\n",
       "6      0002.1999-12-13.farmer       0          0\n",
       "7     0002.2001-02-07.kitchen       0          0\n",
       "8   0002.2001-05-25.SA_and_HP       1          0\n",
       "9          0002.2003-12-18.GP       1          0\n",
       "10         0002.2004-08-01.BG       1          0\n",
       "11   0003.1999-12-10.kaminski       0          0\n",
       "12     0003.1999-12-14.farmer       0          0\n",
       "13       0003.2000-01-17.beck       0          0\n",
       "14    0003.2001-02-08.kitchen       0          0\n",
       "15         0003.2003-12-18.GP       1          0\n",
       "16         0003.2004-08-01.BG       1          0\n",
       "17   0004.1999-12-10.kaminski       0          0\n",
       "18     0004.1999-12-14.farmer       0          0\n",
       "19   0004.2001-04-02.williams       0          0\n",
       "20  0004.2001-06-12.SA_and_HP       1          0\n",
       "21         0004.2004-08-01.BG       1          0\n",
       "22   0005.1999-12-12.kaminski       0          0\n",
       "23     0005.1999-12-14.farmer       0          0\n",
       "24      0005.2000-06-06.lokay       0          0\n",
       "25    0005.2001-02-08.kitchen       0          0\n",
       "26  0005.2001-06-23.SA_and_HP       1          0\n",
       "27         0005.2003-12-18.GP       1          0\n",
       "28   0006.1999-12-13.kaminski       0          0\n",
       "29    0006.2001-02-08.kitchen       0          0\n",
       "30   0006.2001-04-03.williams       0          0\n",
       "31  0006.2001-06-25.SA_and_HP       1          0\n",
       "32         0006.2003-12-18.GP       1          0\n",
       "33         0006.2004-08-01.BG       1          0\n",
       "34   0007.1999-12-13.kaminski       0          0\n",
       "35     0007.1999-12-14.farmer       0          0\n",
       "36       0007.2000-01-17.beck       0          0\n",
       "37    0007.2001-02-09.kitchen       0          0\n",
       "38         0007.2003-12-18.GP       1          0\n",
       "39         0007.2004-08-01.BG       1          0\n",
       "40    0008.2001-02-09.kitchen       0          0\n",
       "41  0008.2001-06-12.SA_and_HP       1          0\n",
       "42  0008.2001-06-25.SA_and_HP       1          0\n",
       "43         0008.2003-12-18.GP       1          0\n",
       "44         0008.2004-08-01.BG       1          0\n",
       "45   0009.1999-12-13.kaminski       0          0\n",
       "46     0009.1999-12-14.farmer       0          0\n",
       "47      0009.2000-06-07.lokay       0          0\n",
       "48    0009.2001-02-09.kitchen       0          0\n",
       "49  0009.2001-06-26.SA_and_HP       1          0\n",
       "50         0009.2003-12-18.GP       1          1\n",
       "51     0010.1999-12-14.farmer       0          0\n",
       "52   0010.1999-12-14.kaminski       0          0\n",
       "53    0010.2001-02-09.kitchen       0          0\n",
       "54  0010.2001-06-28.SA_and_HP       1          0\n",
       "55         0010.2003-12-18.GP       1          0\n",
       "56         0010.2004-08-01.BG       1          0\n",
       "57     0011.1999-12-14.farmer       0          0\n",
       "58  0011.2001-06-28.SA_and_HP       1          0\n",
       "59  0011.2001-06-29.SA_and_HP       1          0\n",
       "60         0011.2003-12-18.GP       1          0\n",
       "61         0011.2004-08-01.BG       1          0\n",
       "62     0012.1999-12-14.farmer       0          0\n",
       "63   0012.1999-12-14.kaminski       0          0\n",
       "64       0012.2000-01-17.beck       0          0\n",
       "65      0012.2000-06-08.lokay       0          0\n",
       "66    0012.2001-02-09.kitchen       0          0\n",
       "67         0012.2003-12-19.GP       1          0\n",
       "68     0013.1999-12-14.farmer       0          0\n",
       "69   0013.1999-12-14.kaminski       0          0\n",
       "70   0013.2001-04-03.williams       0          0\n",
       "71  0013.2001-06-30.SA_and_HP       1          0\n",
       "72         0013.2004-08-01.BG       1          0\n",
       "73   0014.1999-12-14.kaminski       0          0\n",
       "74     0014.1999-12-15.farmer       0          0\n",
       "75    0014.2001-02-12.kitchen       0          0\n",
       "76  0014.2001-07-04.SA_and_HP       1          0\n",
       "77         0014.2003-12-19.GP       1          0\n",
       "78         0014.2004-08-01.BG       1          0\n",
       "79   0015.1999-12-14.kaminski       0          0\n",
       "80     0015.1999-12-15.farmer       0          0\n",
       "81      0015.2000-06-09.lokay       0          0\n",
       "82    0015.2001-02-12.kitchen       0          0\n",
       "83  0015.2001-07-05.SA_and_HP       1          0\n",
       "84         0015.2003-12-19.GP       1          0\n",
       "85     0016.1999-12-15.farmer       0          0\n",
       "86    0016.2001-02-12.kitchen       0          0\n",
       "87  0016.2001-07-05.SA_and_HP       1          0\n",
       "88  0016.2001-07-06.SA_and_HP       1          0\n",
       "89         0016.2003-12-19.GP       1          1\n",
       "90         0016.2004-08-01.BG       1          0\n",
       "91   0017.1999-12-14.kaminski       0          0\n",
       "92       0017.2000-01-17.beck       0          0\n",
       "93   0017.2001-04-03.williams       0          0\n",
       "94         0017.2003-12-18.GP       1          0\n",
       "95         0017.2004-08-01.BG       1          1\n",
       "96         0017.2004-08-02.BG       1          0\n",
       "97   0018.1999-12-14.kaminski       0          0\n",
       "98  0018.2001-07-13.SA_and_HP       1          1\n",
       "99         0018.2003-12-18.GP       1          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "def hw1_4():\n",
    "    !./pNaiveBayes.sh 2 'assistance valium enlargementWithATypo'\n",
    "    \n",
    "    df = pd.read_csv(\"enronemail_1h.txt.output\",sep='\\t')\n",
    "    find_accuracy(df)\n",
    "    with pd.option_context('display.max_rows', 999, 'display.max_columns', 3):\n",
    "        display(df)\n",
    "    return\n",
    "\n",
    "hw1_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1.5\n",
    "-----------\n",
    "### Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh, will classify the email messages by all words present.  To do so, make sure that (a) mapper.py counts all occurrences of all words, and (b) reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Design Summary and Testing \n",
    "\n",
    "\n",
    "1. updated pNaiveBayesh.sh to pass the wordlist and count\n",
    "1. mapper.py outputs data in the following format for each of the chunk created\n",
    "    1. email ID, true class, and word counts\n",
    "       \n",
    "       exmaple :\n",
    "         * 0001.1999-12-10.farmer      0   farm    1   pictures    1   ....\n",
    "         * 0001.1999-12-10.kaminski    0   re  1   you 1   thank   1   ....\n",
    "1. email \"subject\" and \"text\" are combined and searched for matching words for NaiveBayes algorithm\n",
    "1. Used Llaplace smoothing(add one) to avoid divide by zero error . Testing the non laplace results they were not as good. \n",
    "1. Reducer uses log based implementation of NB as given in 13.4 of the book\n",
    "        \n",
    "1. Initial testing was done on the test.tst data similar to that of table 13.1 and matched with the Exercise 13.1 example conditional probabilities\n",
    "         P(Chinese|C) = 3/7\n",
    "         P(Japan|~C) =   2/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "num_countfiles=`echo \"$linesindata/($linesindata/$m+1)+1\" | bc`\n",
    "./reducer.py $num_countfiles \"$wordlist\" $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "# clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper 1.5\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "filename = sys.argv[1]\n",
    "\n",
    "# read in the data and create an output list with email ID, true class, and word counts\n",
    "output = []\n",
    "with open (filename, \"r\") as myfile:                 # for the given file\n",
    "    for line in myfile.readlines():                  # for each line in the file\n",
    "        line_output = []                             # create an empty list for the line output\n",
    "        split_line = string.split(line, maxsplit = 2)  # split the line into three parts\n",
    "#         split_line = string.split(line, sep='\\t')    # split the line into four parts\n",
    "        line_output.append(split_line[0])              # first part is the email ID\n",
    "        line_output.append(split_line[1])              # second part is the true class\n",
    "        text = split_line[2]                           # third part is the email text\n",
    "        word_count = {}                              # create an empty dictionary for counting the words   \n",
    "        for word in text.lower().split():            # for each word in the email text\n",
    "            clean_word = re.sub(r'[^\\w\\s]','',word)  # remove punctuation\n",
    "            if clean_word != \"\":                     # if the cleaned word is not empty\n",
    "                if clean_word not in word_count:     # if the cleaned word is not in word list\n",
    "                    word_count[clean_word] = 1       # set its count to 1\n",
    "                else:                                # otherwise if it is already in the word list  \n",
    "                    word_count[clean_word] += 1      # add 1 to its count\n",
    "        \n",
    "        line_output.append(word_count)               # add the word counts dictionary to the line output  \n",
    "        output.append(line_output)                   # add the line output to the final output\n",
    "\n",
    "# print out tab-delimited data of email ID, true class, and word counts\n",
    "for example in output:\n",
    "    counts = \"\\t\".join([\"%s\\t%s\" % (word, count) for word, count in example[2].items()])\n",
    "    print example[0], \"\\t\", example[1], \"\\t\", counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer 1.5\n",
    "---\n",
    "* Uses Laplace smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# read in the data from the mapper output files and create a list\n",
    "# data = [email_ID, true_class, {word: count}]\n",
    "data = []\n",
    "for i in range(int(sys.argv[1])):         # for each chunk\n",
    "    filename = sys.argv[i+3]              # get the filename for that chunk\n",
    "    with open (filename, \"r\") as myfile:  # open the file\n",
    "        for line in myfile.readlines():   # for each line in the file\n",
    "            line_output = []              # create a list for that line\n",
    "            split_line = string.split(line, maxsplit=2)  # split the line into three parts\n",
    "            line_output.append(split_line[0])  # append the email id\n",
    "            line_output.append(split_line[1])  # append the true class\n",
    "            text = split_line[2].split()       # split the email text\n",
    "            word_count = {}                    # create a dictionary for the word counts   \n",
    "            for j in range(len(text)/2):       # for every other token in the email text\n",
    "                word_count[text[2*j]] = int(text[2*j+1])  # assign the count to the word in the dictionary   \n",
    "            line_output.append(word_count)     # add the wordcount dictionary to the line\n",
    "            data.append(line_output)           # add the line to the data\n",
    "    \n",
    "# create a vocabulary of words from all examples and count occurrences in ham and spam\n",
    "# count total number of hams, total number of spams\n",
    "# vocab = {word:[number of occurrences in ham, number of occurrences in spam]}\n",
    "vocab = {}\n",
    "count_ham = 0\n",
    "count_spam = 0\n",
    "\n",
    "for example in data:\n",
    "    if example[1] == '0':\n",
    "        count_ham += 1\n",
    "    else:\n",
    "        count_spam += 1\n",
    "    for word in example[2]:\n",
    "        if word not in vocab:\n",
    "            if example[1] == '0':\n",
    "                vocab[word] = [example[2][word], 0]\n",
    "            else:\n",
    "                vocab[word] = [0, example[2][word]]\n",
    "        else:\n",
    "            if example[1] == '0':\n",
    "                vocab[word][0] += example[2][word]\n",
    "            else:\n",
    "                vocab[word][1] += example[2][word]\n",
    "\n",
    "# count total number of words in vocab, ham, and spam\n",
    "words_vocab = len(vocab)\n",
    "words_ham = sum(h for h, s in vocab.itervalues()) \n",
    "words_spam = sum(s for h, s in vocab.itervalues()) \n",
    "\n",
    "# calculate prior probabilities of ham and spam\n",
    "prior_Pr_ham = count_ham*1.0/(count_ham + count_spam)\n",
    "prior_Pr_spam = count_spam*1.0/(count_ham + count_spam)\n",
    "\n",
    "# calculate conditional probabilities for each word in vocab for ham and spam using +1 Laplace smoothing  \n",
    "#cond_prob{} = {word:[conditional probability ham, conditional probability spam]}\n",
    "cond_prob = {}\n",
    "for word in vocab:\n",
    "    cond_prob[word] = [(vocab[word][0] + 1)*1.0/(words_ham + words_vocab), \n",
    "                       (vocab[word][1] + 1)*1.0/(words_spam + words_vocab)]\n",
    "\n",
    "# result of predictions\n",
    "results = []\n",
    "\n",
    "# calculate posterior probabilities of ham and spam\n",
    "for example in data:\n",
    "    prediction = []                # create an empty list to hold the prediction\n",
    "    prediction.append(example[0])  # add the email id to the prediction list\n",
    "    prediction.append(example[1])  # add the true class to the prediction list\n",
    "    \n",
    "    # set initial posterior probabilities equal to the prior probabilities\n",
    "    log_post_Pr_ham = np.log(prior_Pr_ham)\n",
    "    log_post_Pr_spam = np.log(prior_Pr_spam)\n",
    "\n",
    "    # calculate posterior probabilities for this example\n",
    "    for word in example[2]:           # for each word in this example's email text\n",
    "        log_post_Pr_ham += example[2][word]*np.log(cond_prob[word][0])   # add the conditional probability of word given ham for each occurrence of word in this example   \n",
    "        log_post_Pr_spam += example[2][word]*np.log(cond_prob[word][1])  # add the conditional probability of word given spam for each occurrence of word in this example  \n",
    "            \n",
    "    # make a prediction for this example\n",
    "    if log_post_Pr_ham >= log_post_Pr_spam:  # if the posterior probability of ham is greater than the posterior of spam\n",
    "        prediction.append('0')  # label this example as ham\n",
    "    else:                       # otherwise\n",
    "        prediction.append('1')  # label this example as spam\n",
    "    results.append(\"\\t\".join(prediction))\n",
    "\n",
    "print \"%s\\t%s\\t%s\" % (\"id\",\"actual\",\"predicted\")\n",
    "for entry_line in results:\n",
    "    print entry_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MapReduce 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001.1999-12-10.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002.1999-12-13.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0002.2001-02-07.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0002.2001-05-25.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0002.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0003.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0003.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0003.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0003.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0003.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0004.1999-12-10.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0004.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0004.2001-04-02.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0004.2001-06-12.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0004.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0005.1999-12-12.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0005.1999-12-14.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0005.2000-06-06.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0005.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0005.2001-06-23.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0005.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0006.1999-12-13.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0006.2001-02-08.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0013.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0013.2001-06-30.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0013.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0014.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0014.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0014.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0014.2001-07-04.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0014.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0014.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0015.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0015.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0015.2000-06-09.lokay</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0015.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0015.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0015.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0016.1999-12-15.farmer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0016.2001-02-12.kitchen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0016.2001-07-05.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0016.2001-07-06.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0016.2003-12-19.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0016.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0017.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0017.2000-01-17.beck</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0017.2001-04-03.williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0017.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0017.2004-08-01.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0017.2004-08-02.BG</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0018.1999-12-14.kaminski</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0018.2001-07-13.SA_and_HP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0018.2003-12-18.GP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            id actual  predicted\n",
       "0       0001.1999-12-10.farmer      0          0\n",
       "1     0001.1999-12-10.kaminski      0          0\n",
       "2         0001.2000-01-17.beck      0          0\n",
       "3        0001.2000-06-06.lokay      0          0\n",
       "4      0001.2001-02-07.kitchen      0          0\n",
       "5     0001.2001-04-02.williams      0          0\n",
       "6       0002.1999-12-13.farmer      0          0\n",
       "7      0002.2001-02-07.kitchen      0          0\n",
       "8    0002.2001-05-25.SA_and_HP      1          1\n",
       "9           0002.2003-12-18.GP      1          1\n",
       "10          0002.2004-08-01.BG      1          1\n",
       "11    0003.1999-12-10.kaminski      0          0\n",
       "12      0003.1999-12-14.farmer      0          0\n",
       "13        0003.2000-01-17.beck      0          0\n",
       "14     0003.2001-02-08.kitchen      0          0\n",
       "15          0003.2003-12-18.GP      1          1\n",
       "16          0003.2004-08-01.BG      1          1\n",
       "17    0004.1999-12-10.kaminski      0          0\n",
       "18      0004.1999-12-14.farmer      0          0\n",
       "19    0004.2001-04-02.williams      0          0\n",
       "20   0004.2001-06-12.SA_and_HP      1          1\n",
       "21          0004.2004-08-01.BG      1          1\n",
       "22    0005.1999-12-12.kaminski      0          0\n",
       "23      0005.1999-12-14.farmer      0          0\n",
       "24       0005.2000-06-06.lokay      0          0\n",
       "25     0005.2001-02-08.kitchen      0          0\n",
       "26   0005.2001-06-23.SA_and_HP      1          1\n",
       "27          0005.2003-12-18.GP      1          1\n",
       "28    0006.1999-12-13.kaminski      0          0\n",
       "29     0006.2001-02-08.kitchen      0          0\n",
       "..                         ...    ...        ...\n",
       "71    0013.2001-04-03.williams      0          0\n",
       "72   0013.2001-06-30.SA_and_HP      1          1\n",
       "73          0013.2004-08-01.BG      1          1\n",
       "74    0014.1999-12-14.kaminski      0          0\n",
       "75      0014.1999-12-15.farmer      0          0\n",
       "76     0014.2001-02-12.kitchen      0          0\n",
       "77   0014.2001-07-04.SA_and_HP      1          1\n",
       "78          0014.2003-12-19.GP      1          1\n",
       "79          0014.2004-08-01.BG      1          1\n",
       "80    0015.1999-12-14.kaminski      0          0\n",
       "81      0015.1999-12-15.farmer      0          0\n",
       "82       0015.2000-06-09.lokay      0          0\n",
       "83     0015.2001-02-12.kitchen      0          0\n",
       "84   0015.2001-07-05.SA_and_HP      1          1\n",
       "85          0015.2003-12-19.GP      1          1\n",
       "86      0016.1999-12-15.farmer      0          0\n",
       "87     0016.2001-02-12.kitchen      0          0\n",
       "88   0016.2001-07-05.SA_and_HP      1          1\n",
       "89   0016.2001-07-06.SA_and_HP      1          1\n",
       "90          0016.2003-12-19.GP      1          1\n",
       "91          0016.2004-08-01.BG      1          1\n",
       "92    0017.1999-12-14.kaminski      0          0\n",
       "93        0017.2000-01-17.beck      0          0\n",
       "94    0017.2001-04-03.williams      0          0\n",
       "95          0017.2003-12-18.GP      1          1\n",
       "96          0017.2004-08-01.BG      1          1\n",
       "97          0017.2004-08-02.BG      1          1\n",
       "98    0018.1999-12-14.kaminski      0          0\n",
       "99   0018.2001-07-13.SA_and_HP      1          1\n",
       "100         0018.2003-12-18.GP      1          1\n",
       "\n",
       "[101 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "def hw1_5():\n",
    "    !./pNaiveBayes.sh 2 '*'\n",
    "    \n",
    "    df = pd.read_csv(\"enronemail_1h.txt.output\",sep='\\t')\n",
    "    find_accuracy(df)\n",
    "    display(df)\n",
    "    return\n",
    "\n",
    "hw1_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## HW1.6\n",
    "-----------\n",
    "## Benchmark your code with the Python SciKit-Learn implementation of Naive Bayes.\n",
    "HW1.6 Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes\n",
    "\n",
    "It always a good idea to test your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "Lets define  Training error = misclassification rate with respect to a training set. It is more formally defined here:\n",
    "\n",
    "Let DF represent the training set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "### 1.6.1 Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "\n",
    "### 1.6.2 Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error \n",
    "\n",
    "### 1.6.3 Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error \n",
    "\n",
    "### 1.6.4  Please prepare a table to present your results\n",
    "\n",
    "### 1.6.5 Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your  Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)\n",
    "\n",
    "### 1.6.6 Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer 1.6\n",
    "Reducer 1.6 outputs data in the format needed by the Naive Bayes functions in SciKit-Learn\n",
    "\n",
    "* Reducer output is changed for use in scilearn input . # create a new output file with full vocabulary word counts for each email and first column is the true class used in trianing\n",
    "\n",
    "Exmaple:\n",
    "0   1   1   1   1   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# read in the data from the mapper output files\n",
    "data = []\n",
    "for i in range(int(sys.argv[1])):         # for each chunk\n",
    "    filename = sys.argv[i+3]              # get the filename for that chunk\n",
    "    with open (filename, \"r\") as myfile:  # open the file\n",
    "        for line in myfile.readlines():   # for each line in the file\n",
    "            line_output = []              # create a list for that line\n",
    "            split_line = string.split(line, maxsplit=2)  # split the line into three parts\n",
    "            line_output.append(split_line[0])  # append the email id\n",
    "            line_output.append(split_line[1])  # append the true class\n",
    "            text = split_line[2].split()       # split the email text\n",
    "            word_count = {}                    # create a dictionary for the word counts   \n",
    "            for j in range(len(text)/2):       # for every other token in the email text\n",
    "                word_count[text[2*j]] = int(text[2*j+1])  # assign the count to the word in the dictionary   \n",
    "            line_output.append(word_count)     # add the wordcount dictionary to the line\n",
    "            data.append(line_output)           # add the line to the data\n",
    "\n",
    "# create a vocabulary of all words that occur in the Enron email file with their counts\n",
    "vocab = []\n",
    "for example in data:\n",
    "    for word in example[2]:\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "            \n",
    "# create a new output file with full vocabulary word counts for each example\n",
    "for example in data:                             # for each example in the data\n",
    "    word_count = []                              # create a word count list\n",
    "    for word in vocab:                           # for each word in the vocabulary\n",
    "        if word in example[2]:                   # if the word is in the example email text\n",
    "            word_count.append(example[2][word])  # get the count of that word in the example\n",
    "        else:                                    # if the word is not in the example email text  \n",
    "            word_count.append(0)                 # set the word's count to 0\n",
    "\n",
    "    # print out tab-delimited data of true class and word counts\n",
    "    counts = \"\\t\".join([\"%i\" %(count) for count in word_count])\n",
    "    print example[1], \"\\t\", counts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Reducer 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 20 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1-1.6.2:  SciKit-Learn Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "# import data created by Reducer 1.6\n",
    "data = []\n",
    "with open('enronemail_1h.txt.output', 'rb') as csvfile:\n",
    "    datareader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for line in datareader:\n",
    "        data.append(line)\n",
    "    \n",
    "# create training data\n",
    "y = []\n",
    "X = []\n",
    "for example in data:\n",
    "    counts = []\n",
    "    try:\n",
    "        int(example[0])\n",
    "    except:\n",
    "        continue\n",
    "    y.append(int(example[0]))\n",
    "    for i in range(1, len(example),1):\n",
    "        counts.append(int(example[i]))\n",
    "    X.append(counts)\n",
    "        \n",
    "# Multinomial Naive Bayes Model\n",
    "mnb = MultinomialNB(fit_prior = True) # instantiate the Naive Bayes model\n",
    "mnb.fit(X,y)\n",
    "mnb_training_error = 1 - mnb.score(X,y)\n",
    "print mnb_training_error\n",
    "\n",
    "# Bernoulli Naive Bayes Model\n",
    "bnb = BernoulliNB(binarize = 0.0, fit_prior = True) # instantiate the Naive Bayes model\n",
    "bnb.fit(X,y)\n",
    "bnb_training_error = 1 - bnb.score(X,y)\n",
    "print bnb_training_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Rewrite Reducer 1.5\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# read in the data from the mapper output files and create a list\n",
    "# data = [email_ID, true_class, {word: count}]\n",
    "data = []\n",
    "for i in range(int(sys.argv[1])):         # for each chunk\n",
    "    filename = sys.argv[i+3]              # get the filename for that chunk\n",
    "    with open (filename, \"r\") as myfile:  # open the file\n",
    "        for line in myfile.readlines():   # for each line in the file\n",
    "            line_output = []              # create a list for that line\n",
    "            split_line = string.split(line, maxsplit=2)  # split the line into three parts\n",
    "            line_output.append(split_line[0])  # append the email id\n",
    "            line_output.append(split_line[1])  # append the true class\n",
    "            text = split_line[2].split()       # split the email text\n",
    "            word_count = {}                    # create a dictionary for the word counts   \n",
    "            for j in range(len(text)/2):       # for every other token in the email text\n",
    "                word_count[text[2*j]] = int(text[2*j+1])  # assign the count to the word in the dictionary   \n",
    "            line_output.append(word_count)     # add the wordcount dictionary to the line\n",
    "            data.append(line_output)           # add the line to the data\n",
    "    \n",
    "# create a vocabulary of words from all examples and count occurrences in ham and spam\n",
    "# count total number of hams, total number of spams\n",
    "# vocab = {word:[number of occurrences in ham, number of occurrences in spam]}\n",
    "vocab = {}\n",
    "count_ham = 0\n",
    "count_spam = 0\n",
    "\n",
    "for example in data:  # data has rows  in the format [ID,true Class, text]\n",
    "    if example[1] == '0': #if true class is ham\n",
    "        count_ham += 1  #increemnt ham count\n",
    "    else:\n",
    "        count_spam += 1 # increment spam count\n",
    "    for word in example[2]: # for each word in the text\n",
    "        if word not in vocab: # and if the word in the vocabilary\n",
    "            if example[1] == '0': #if ham\n",
    "                vocab[word] = [example[2][word], 0] # initialize with count according to if it was ham or spam\n",
    "            else:                                   # swap the count position in the tupple\n",
    "                vocab[word] = [0, example[2][word]] \n",
    "        else:\n",
    "            if example[1] == '0':\n",
    "                vocab[word][0] += example[2][word] # add to the right counter ,here for ham\n",
    "            else:\n",
    "                vocab[word][1] += example[2][word] # spam\n",
    "\n",
    "# count total number of words in vocab, ham, and spam\n",
    "words_vocab = len(vocab)\n",
    "words_ham = sum(h for h, s in vocab.itervalues()) \n",
    "words_spam = sum(s for h, s in vocab.itervalues()) \n",
    "\n",
    "# calculate prior probabilities of ham and spam\n",
    "prior_Pr_ham = count_ham*1.0/(count_ham + count_spam)\n",
    "prior_Pr_spam = count_spam*1.0/(count_ham + count_spam)\n",
    "\n",
    "# calculate conditional probabilities for each word in vocab for ham and spam using +1 Laplace smoothing  \n",
    "#cond_prob{} = {word:[conditional probability ham, conditional probability spam]}\n",
    "cond_prob = {}\n",
    "for word in vocab:\n",
    "    cond_prob[word] = [(vocab[word][0] + 1)*1.0/(words_ham + words_vocab), \n",
    "                       (vocab[word][1] + 1)*1.0/(words_spam + words_vocab)]\n",
    "\n",
    "# output predictions\n",
    "output = []\n",
    "\n",
    "# calculate posterior probabilities of ham and spam\n",
    "for example in data:\n",
    "    prediction = []                \n",
    "    prediction.append(example[0])  \n",
    "    prediction.append(example[1])  \n",
    "    \n",
    "    # set initial posterior probabilities equal to the prior probabilities\n",
    "    log_post_Pr_ham = np.log(prior_Pr_ham)\n",
    "    log_post_Pr_spam = np.log(prior_Pr_spam)\n",
    "\n",
    "    # calculate posterior probabilities for this example\n",
    "    for word in example[2]:           # for each word in this example's email text\n",
    "        log_post_Pr_ham += example[2][word]*np.log(cond_prob[word][0])   # add the conditional probability of word given ham for each occurrence of word in this example   \n",
    "        log_post_Pr_spam += example[2][word]*np.log(cond_prob[word][1])  # add the conditional probability of word given spam for each occurrence of word in this example  \n",
    "            \n",
    "    # make a prediction for this example\n",
    "    if log_post_Pr_ham >= log_post_Pr_spam:  # if the posterior probability of ham is greater than the posterior of spam\n",
    "        prediction.append('0')  # label this example as ham\n",
    "    else:                       # otherwise\n",
    "        prediction.append('1')  # label this example as spam\n",
    "        \n",
    "    output.append(prediction)   # add this prediction to the final output\n",
    "\n",
    "\n",
    "#test the accuracy of this predictor\n",
    "count = 0\n",
    "correct = 0\n",
    "for line in output:\n",
    "    if line[1] == line[2]:\n",
    "        correct += 1\n",
    "    count += 1\n",
    "training_error = 1 - correct*1.0/count\n",
    "print \"training_error\"\n",
    "print training_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3:  Rerun MapReduce 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   training_error\n",
       "0        0.009901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "!./pNaiveBayes.sh 20 \"assistance\"\n",
    "\n",
    "df = pd.read_csv(\"enronemail_1h.txt.output\",sep='\\t',header=0)\n",
    "display(df)\n",
    "mr_training_error=round(df['training_error'][0],2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.4:  Table of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Multinomial Naive Bayes</th>\n",
       "      <th>Bernoulli Naive Bayes</th>\n",
       "      <th>MapReduce 1.5 Naive Bayes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Multinomial Naive Bayes Bernoulli Naive Bayes MapReduce 1.5 Naive Bayes\n",
       "0                    0.00                   0.2                     0.010"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [[str(mnb_training_error) + '0'], [bnb_training_error], [str(mr_training_error) + '0']]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "df =pd.DataFrame(data)\n",
    "dft=df.transpose()\n",
    "dft.columns =['Multinomial Naive Bayes', 'Bernoulli Naive Bayes', 'MapReduce 1.5 Naive Bayes']\n",
    "display(dft)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.5:  MapReduce vs. SciKit-Learn\n",
    "There was *no difference in the training error* between the MapReduce implementation in HW1.5 and the Multinomial Naive Bayes implementation using SciKit-Learn. \n",
    "\n",
    "I think using following are some of the reasons for getting 100% accuracy \n",
    "  * the laplace smoothing which I used for HW1.5 and Sci-learng uses\n",
    "  * use of the training set as test set gave us the 100% accuracy,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.6:  Multinomial vs. Bernoulli Naive Bayes\n",
    "\n",
    "A key difference between Multinomial(MNB) and Bernoulli Naive Bayes(BNB) is that MNB counts the occurance of a word in the document and disregards the number of times the word appears in a document.\n",
    "Since some information is lost, we can expect BNB to perform less than MNB for our test examples.\n",
    "\n",
    "A good example may be table 13.1 from our text where the fifth entry is\n",
    "\"Chinese Chinese Chinese Tokyo Japan\"\n",
    "\n",
    "In MNB the word Chinese appearing 3 times increases its chance of indentified as class \"yes\" whereas BNB will consider only one Chinese and will identify it as a Class belonging to \"no\" ( related to Japan)\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
